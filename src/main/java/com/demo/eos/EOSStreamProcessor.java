package com.demo.eos;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.KafkaException;
import org.apache.kafka.common.TopicPartition;

import java.time.Duration;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Random;

public class EOSStreamProcessor {
  public static void main(String[] args) {
    var consumerProps = Config.getConsumerProps(Config.CONSUMER_GROUP_ID, "read_committed");
    var producerProps = Config.getProducerProps(Config.TRANSACTIONAL_ID);

    var consumer = new KafkaConsumer<String, String>(consumerProps);
    var producer = new KafkaProducer<String, String>(producerProps);

    producer.initTransactions();
    consumer.subscribe(Collections.singletonList(Config.INPUT_TOPIC));

    System.out.println("Starting processor. I will simulate random crashes!");

    while (true) {
      var records = consumer.poll(Duration.ofMillis(100));
      if (!records.isEmpty()) {
        try {
          producer.beginTransaction();

          Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();

          for (ConsumerRecord<String, String> record : records) {
            // 1. Process Logic: Multiply by 2
            int number = Integer.parseInt(record.value());
            int result = number * 2;

            System.out.println("Processing: " + number + " -> " + result);

            // 2. Produce to Output
            producer.send(new ProducerRecord<>(Config.OUTPUT_TOPIC, record.key(), String.valueOf(result)));

            // Track offset for commit
            currentOffsets.put(
                new TopicPartition(record.topic(), record.partition()),
                new OffsetAndMetadata(record.offset() + 1));

            // 3. CRASH SIMULATION!
            if (new Random().nextDouble() < 0.3) {
              throw new RuntimeException("ðŸ”¥ SIMULATED CRASH! ðŸ”¥");
            }
          }

          // 4. Commit Offsets to Transaction
          producer.sendOffsetsToTransaction(currentOffsets, consumer.groupMetadata());

          // 5. Commit Transaction
          producer.commitTransaction();
          System.out.println("âœ… Transaction Committed");

        } catch (Exception e) {
          System.err.println("âŒ ERROR: " + e.getMessage());
          producer.abortTransaction();
          // In a real app, you'd restart the valid producer or simple reset.
          // Here we continue loop to show recovery.
          // Just need to seek!
          // Actually, Kafka consumer auto-reset is complex.
          // Simplest recovery: close and restart, or Seek to COMMITTED.

          // For demo simplicity, we just assume next poll will re-fetch if not committed?
          // No, consumer maintains in-memory position. We MUST seek/reset.
          // Let's rely on standard re-poll or seek logic.
          // Since we aborted, the transaction is gone.
          // We need to re-position consumer to last committed.
          // For simplicity in this demo, we exit to force manual restart or handle it.
          // Let's just reset position.

          // Force restart loop logic roughly:
        }
      }
    }
  }
}
